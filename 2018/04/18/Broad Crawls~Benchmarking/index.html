<!DOCTYPE html>
<html>
    <!-- title -->




<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" >
    <meta name="author" content="Hu3sky">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Hu3sky">
    <meta name="keywords" content="Hu3sky | Hu3sky">
    <meta name="description" content="">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
    <title>Hu3sky&#39;s blog</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s 1;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href= /css/style.css?v=20180824 as="style" onload="this.onload=null;this.rel='stylesheet'" />
    <link rel="stylesheet" href= /css/mobile.css?v=20180824 media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href= "/tupian/a.ico" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script" />
    <link rel="preload" href="/scripts/main.js" as="script" />
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
</head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Hu3sky&#39;s blog</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name"></a>
            </div>
    </div>
    
    <a class="home-link" href=/>Hu3sky's blog</a>
</header>
    <div class="wrapper">
        <div class="site-intro" style=








height:50vh;

>
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/tupian/456.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            [Untitled Post]
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                
                    <div class="post-intro-read">
                        <span>Word count: <span class="post-count">10,555</span> / Reading time: <span class="post-count">42 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2018/04/18</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h1 id="通用爬虫-Broad-Crawls"><a href="#通用爬虫-Broad-Crawls" class="headerlink" title="通用爬虫(Broad Crawls)"></a>通用爬虫(Broad Crawls)</h1><p>Scrapy默认对特定爬取进行优化。这些站点一般被一个单独的Scrapy spider进行处理， 不过这并不是必须或要求的(例如，也有通用的爬虫能处理任何给定的站点)。</p>
<p>除了这种爬取完某个站点或没有更多请求就停止的”专注的爬虫”，还有一种通用的爬取类型，其能爬取大量(甚至是无限)的网站， 仅仅受限于时间或其他的限制。 这种爬虫叫做”通用爬虫(broad crawls)”，一般用于搜索引擎。</p>
<p>通用爬虫一般有以下通用特性:</p>
<ul>
<li>其爬取大量(一般来说是无限)的网站而不是特定的一些网站。</li>
<li>其不会将整个网站都爬取完毕，因为这十分不实际(或者说是不可能)完成的。相反，其会限制爬取的时间及数量。</li>
<li>其在逻辑上十分简单(相较于具有很多提取规则的复杂的spider)，数据会在另外的阶段进行后处理(post-processed)</li>
<li>其并行爬取大量网站以避免被某个网站的限制所限制爬取的速度(为表示尊重，每个站点爬取速度很慢但同时爬取很多站点)。</li>
</ul>
<p>正如上面所述，Scrapy默认设置是对特定爬虫做了优化，而不是通用爬虫。不过， 鉴于其使用了异步架构，Scrapy对通用爬虫也十分适用。 本篇文章总结了一些将Scrapy作为通用爬虫所需要的技巧， 以及相应针对通用爬虫的Scrapy设定的一些建议。</p>
<h2 id="增加并发"><a href="#增加并发" class="headerlink" title="增加并发"></a>增加并发</h2><p>并发是指同时处理的request的数量。其有全局限制和局部(每个网站)的限制。</p>
<p>Scrapy默认的全局并发限制对同时爬取大量网站的情况并不适用，因此您需要增加这个值。 增加多少取决于您的爬虫能占用多少CPU。 一般开始可以设置为<code>100</code> 。不过最好的方式是做一些测试，获得Scrapy进程占取CPU与并发数的关系。 为了优化性能，您应该选择一个能使CPU占用率在80%-90%的并发数。</p>
<p>增加全局并发数:</p>
<p><code>CONCURRENT_REQUESTS = 100</code></p>
<h2 id="增加Twisted-IO线程池的最大大小"><a href="#增加Twisted-IO线程池的最大大小" class="headerlink" title="增加Twisted IO线程池的最大大小"></a>增加Twisted IO线程池的最大大小</h2><p>目前，Scrapy以使用线程池的阻塞方式进行DNS解析。在并发级别较高的情况下，爬网可能会很慢，甚至会导致DNS解析器超时失败。增加处理DNS查询的线程数的可能解决方案。DNS队列的处理速度将更快，加快建立连接和整体爬行。</p>
<p>要增加最大线程池大小，请使用：</p>
<p><code>REACTOR_THREADPOOL_MAXSIZE  =  20</code></p>
<h2 id="设置你自己的DNS"><a href="#设置你自己的DNS" class="headerlink" title="设置你自己的DNS"></a>设置你自己的DNS</h2><p>如果您有多个爬网流程和单个中央DNS，它可能会像DNS服务器上的DoS攻击一样，从而导致整个网络变慢甚至阻塞您的计算机。为了避免这种情况，您可以使用本地缓存设置您自己的DNS服务器，并将其上传到OpenDNS或Verizon等大型DNS。</p>
<h2 id="降低log级别"><a href="#降低log级别" class="headerlink" title="降低log级别"></a>降低log级别</h2><p>当进行通用爬取时，一般您所注意的仅仅是爬取的速率以及遇到的错误。 Scrapy使用<code>INFO</code>log级别来报告这些信息。为了减少CPU使用率(及记录log存储的要求), 在生产环境中进行通用爬取时您不应该使用<code>DEBUG</code>log级别。 不过在开发的时候使用<code>DEBUG</code>应该还能接受。</p>
<p>设置Log级别:</p>
<p><code>LOG_LEVEL = &#39;INFO&#39;</code></p>
<h2 id="禁止cookies"><a href="#禁止cookies" class="headerlink" title="禁止cookies"></a>禁止cookies</h2><p>除非您 真的 需要，否则请禁止cookies。在进行通用爬取时cookies并不需要， (搜索引擎则忽略cookies)。禁止cookies能减少CPU使用率及Scrapy爬虫在内存中记录的踪迹，提高性能。</p>
<p>禁止cookies:</p>
<p><code>COOKIES_ENABLED = False</code></p>
<h2 id="禁止重试"><a href="#禁止重试" class="headerlink" title="禁止重试"></a>禁止重试</h2><p>对失败的HTTP请求进行重试会减慢爬取的效率，尤其是当站点响应很慢(甚至失败)时， 访问这样的站点会造成超时并重试多次。这是不必要的，同时也占用了爬虫爬取其他站点的能力。</p>
<p>禁止重试:</p>
<p><code>RETRY_ENABLED = False</code></p>
<h2 id="减小下载超时"><a href="#减小下载超时" class="headerlink" title="减小下载超时"></a>减小下载超时</h2><p>如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。</p>
<p>减小下载超时:</p>
<p><code>DOWNLOAD_TIMEOUT = 15</code></p>
<h2 id="禁止重定向"><a href="#禁止重定向" class="headerlink" title="禁止重定向"></a>禁止重定向</h2><p>除非您对跟进重定向感兴趣，否则请考虑关闭重定向。 当进行通用爬取时，一般的做法是保存重定向的地址，并在之后的爬取进行解析。 这保证了每批爬取的request数目在一定的数量， 否则重定向循环可能会导致爬虫在某个站点耗费过多资源。</p>
<p>关闭重定向:</p>
<p><code>REDIRECT_ENABLED = False</code></p>
<h2 id="启用-“Ajax-Crawlable-Pages”-爬取"><a href="#启用-“Ajax-Crawlable-Pages”-爬取" class="headerlink" title="启用 “Ajax Crawlable Pages” 爬取"></a>启用 “Ajax Crawlable Pages” 爬取</h2><p>有些站点(基于2013年的经验数据，之多有1%)声明其为 <a href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started" target="_blank" rel="noopener">ajax crawlable</a> 。 这意味着该网站提供了原本只有ajax获取到的数据的纯HTML版本。 网站通过两种方法声明:</p>
<p>在url中使用<code>#!</code> - 这是默认的方式;<br>使用特殊的meta标签 - 这在”main”, “index” 页面中使用。<br>Scrapy自动解决(1)；解决(2)您需要启用<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/downloader-middleware.html#ajaxcrawl-middleware" target="_blank" rel="noopener">AjaxCrawlMiddleware:</a></p>
<p><code>AJAXCRAWL_ENABLED = True</code><br>通用爬取经常抓取大量的 “index” 页面； AjaxCrawlMiddleware能帮助您正确地爬取。 由于有些性能问题，且对于特定爬虫没有什么意义，该中间默认关闭。</p>
<h1 id="借助Firefox来爬取"><a href="#借助Firefox来爬取" class="headerlink" title="借助Firefox来爬取"></a>借助Firefox来爬取</h1><p>这里介绍一些使用Firefox进行爬取的点子及建议，以及一些帮助爬取的Firefox实用插件。</p>
<h2 id="在浏览器中检查DOM的注意事项"><a href="#在浏览器中检查DOM的注意事项" class="headerlink" title="在浏览器中检查DOM的注意事项"></a>在浏览器中检查DOM的注意事项</h2><p>Firefox插件操作的是活动的浏览器DOM(live browser DOM)，这意味着当您检查网页源码的时候， 其已经不是原始的HTML，而是经过浏览器清理并执行一些Javascript代码后的结果。 Firefox是个典型的例子，其会在table中添加 <code>&lt;tbody&gt;</code> 元素。 而Scrapy相反，其并不修改原始的HTML，因此如果在XPath表达式中使用 <code>&lt;tbody&gt;</code> ，您将获取不到任何数据。</p>
<p>所以，当XPath配合Firefox使用时您需要记住以下几点:</p>
<ul>
<li>当检查DOM来查找Scrapy使用的XPath时，禁用Firefox的Javascrpit。</li>
<li>永远不要用完整的XPath路径。使用相对及基于属性(例如 <code>id</code> ， <code>class</code> ， <code>width</code> 等)的路径 或者具有区别性的特性例如 <code>contains(@href, &#39;image&#39;)</code> 。</li>
<li>永远不要在XPath表达式中加入 <code>&lt;tbody&gt;</code> 元素，除非您知道您在做什么</li>
</ul>
<h2 id="对爬取有帮助的实用Firefox插件"><a href="#对爬取有帮助的实用Firefox插件" class="headerlink" title="对爬取有帮助的实用Firefox插件"></a>对爬取有帮助的实用Firefox插件</h2><h3 id="Firebug"><a href="#Firebug" class="headerlink" title="Firebug"></a>Firebug</h3><p>Firebug 是一个在web开发者间很著名的工具，其对抓取也十分有用。 尤其是 检查元素(Inspect Element) 特性对构建抓取数据的XPath十分方便。 当移动鼠标在页面元素时，您能查看相应元素的HTML源码。</p>
<p>查看 使用Firebug进行爬取 ，了解如何配合Scrapy使用Firebug的详细教程。</p>
<h3 id="XPather"><a href="#XPather" class="headerlink" title="XPather"></a>XPather</h3><p>XPather 能让你在页面上直接测试XPath表达式。</p>
<h3 id="XPath-Checker"><a href="#XPath-Checker" class="headerlink" title="XPath Checker"></a>XPath Checker</h3><p>XPath Checker 是另一个用于测试XPath表达式的Firefox插件。</p>
<h3 id="Tamper-Data"><a href="#Tamper-Data" class="headerlink" title="Tamper Data"></a>Tamper Data</h3><p>Tamper Data 是一个允许您查看及修改Firefox发送的header的插件。Firebug能查看HTTP header，但无法修改。</p>
<h3 id="Firecookie"><a href="#Firecookie" class="headerlink" title="Firecookie"></a>Firecookie</h3><p>Firecookie 使得查看及管理cookie变得简单。您可以使用这个插件来创建新的cookie， 删除存在的cookie，查看当前站点的cookie，管理cookie的权限及其他功能。</p>
<h1 id="使用Firebug进行爬取"><a href="#使用Firebug进行爬取" class="headerlink" title="使用Firebug进行爬取"></a>使用Firebug进行爬取</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>本文档介绍了如何使用 <a href="http://getfirebug.com/" target="_blank" rel="noopener">Firebug</a> (一个Firefox的插件)来使得爬取更为简单，有趣。 更多有意思的Firefox插件请参考 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/firefox.html#topics-firefox-addons" target="_blank" rel="noopener">对爬取有帮助的实用Firefox插件</a> 。 使用Firefox插件检查页面需要有些注意事项: <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/firefox.html#topics-firefox-livedom" target="_blank" rel="noopener">在浏览器中检查DOM的注意事项</a> 。</p>
<p>在本样例中将展现如何使用 <a href="http://getfirebug.com/" target="_blank" rel="noopener">Firebug</a> 从 <a href="http://directory.google.com/" target="_blank" rel="noopener">Google Directory</a> 来爬取数据。<a href="http://directory.google.com/" target="_blank" rel="noopener">Google Directory</a> 包含了 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html#intro-tutorial" target="_blank" rel="noopener">入门教程</a> 里所使用的 <a href="http://www.dmoz.org/" target="_blank" rel="noopener">Open Directory Project</a> 中一样的数据，不过有着不同的结构。</p>
<p>Firebug提供了非常实用的 <a href="http://www.youtube.com/watch?v=-pT_pDe54aA" target="_blank" rel="noopener">检查元素</a> 功能。该功能允许您将鼠标悬浮在不同的页面元素上， 显示相应元素的HTML代码。否则，您只能十分痛苦的在HTML的body中手动搜索标签。</p>
<p>在下列截图中，您将看到 <a href="http://www.youtube.com/watch?v=-pT_pDe54aA" target="_blank" rel="noopener">检查元素</a> 的执行效果。</p>
<p><img src="C:\Users\asus\Desktop\Hu3sky.github.io\source\翻译图片\firebug1.png" alt="firebug1"></p>
<p>首先我们能看到目录根据种类进行分类的同时，还划分了子类。</p>
<p>不过，看起来子类还有更多的子类，而不仅仅是页面显示的这些，所以我们接着查找:</p>
<p><img src="C:\Users\asus\Desktop\Hu3sky.github.io\source\翻译图片\firebug2.png" alt="firebug2"></p>
<p>正如路径的概念那样，子类包含了其他子类的链接，同时也链接到实际的网站中。</p>
<h2 id="获取到跟进-follow-的链接"><a href="#获取到跟进-follow-的链接" class="headerlink" title="获取到跟进(follow)的链接"></a>获取到跟进(follow)的链接</h2><p>查看路径的URL，我们可以看到URL的通用模式(pattern):</p>
<blockquote>
<p><a href="http://directory.google.com/Category/Subcategory/Another_Subcategory" target="_blank" rel="noopener">http://directory.google.com/Category/Subcategory/Another_Subcategory</a></p>
</blockquote>
<p>了解到这个消息，我们可以构建一个跟进的链接的正则表达式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">directory\.google\.com/[A-Z][a-zA-Z_/]+$</span><br></pre></td></tr></table></figure>
<p>因此，根据这个表达式，我们创建第一个爬取规则:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rule(LinkExtractor(allow=&apos;directory.google.com/[A-Z][a-zA-Z_/]+$&apos;, ),</span><br><span class="line">    &apos;parse_category&apos;,</span><br><span class="line">    follow=True,</span><br><span class="line">),</span><br></pre></td></tr></table></figure>
<p><a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.contrib.spiders.Rule" target="_blank" rel="noopener"><code>Rule</code></a> 对象指导基于 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.contrib.spiders.CrawlSpider" target="_blank" rel="noopener"><code>CrawlSpider</code></a> 的spider如何跟进目录链接。<code>parse_category</code> 是spider的方法，用于从页面中处理也提取数据。</p>
<p>spider的代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.contrib.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.contrib.spiders import CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line">class GoogleDirectorySpider(CrawlSpider):</span><br><span class="line">    name = &apos;directory.google.com&apos;</span><br><span class="line">    allowed_domains = [&apos;directory.google.com&apos;]</span><br><span class="line">    start_urls = [&apos;http://directory.google.com/&apos;]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=&apos;directory\.google\.com/[A-Z][a-zA-Z_/]+$&apos;),</span><br><span class="line">            &apos;parse_category&apos;, follow=True,</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_category(self, response):</span><br><span class="line">        # write the category page data extraction code here</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>
<h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>现在我们来编写提取数据的代码。</p>
<p>在Firebug的帮助下，我们将查看一些包含网站链接的网页(以 <a href="http://directory.google.com/Top/Arts/Awards/" target="_blank" rel="noopener">http://directory.google.com/Top/Arts/Awards/</a> 为例)， 找到使用 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/selectors.html#topics-selectors" target="_blank" rel="noopener">Selectors</a>提取链接的方法。 我们也将使用 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/shell.html#topics-shell" target="_blank" rel="noopener">Scrapy shell</a> 来测试得到的XPath表达式，确保表达式工作符合预期。</p>
<p><img src="C:\Users\asus\Desktop\Hu3sky.github.io\source\翻译图片\firebug3.png" alt="firebug3"></p>
<p>正如您所看到的那样，页面的标记并不是十分明显: 元素并不包含 <code>id</code> ， <code>class</code> 或任何可以区分的属性。所以我们将使用等级槽(rank bar)作为指示点来选择提取的数据，创建XPath。</p>
<p>使用Firebug，我们可以看到每个链接都在 <code>td</code> 标签中。该标签存在于同时(在另一个 <code>td</code>)包含链接的等级槽(ranking bar)的 <code>tr</code> 中。</p>
<p>所以我们选择等级槽(ranking bar)，接着找到其父节点(<code>tr</code>)，最后是(包含我们要爬取数据的)链接的 <code>td</code> 。</p>
<p>对应的XPath:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//td[descendant::a[contains(@href, &quot;#pagerank&quot;)]]/following-sibling::td//a</span><br></pre></td></tr></table></figure>
<p>使用 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/shell.html#topics-shell" target="_blank" rel="noopener">Scrapy终端</a> 来测试这些复杂的XPath表达式，确保其工作符合预期。</p>
<p>简单来说，该表达式会查找等级槽的 <code>td</code> 元素，接着选择所有 <code>td</code> 元素，该元素拥有子孙 <code>a</code> 元素，且 <code>a</code> 元素的属性 <code>href</code> 包含字符串 <code>#pagerank</code>。</p>
<p>当然，这不是唯一的XPath，也许也不是选择数据的最简单的那个。 其他的方法也可能是，例如，选择灰色的链接的 <code>font</code> 标签。</p>
<p>最终，我们编写 <code>parse_category()</code> 方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def parse_category(self, response):</span><br><span class="line"></span><br><span class="line">    # The path to website links in directory page</span><br><span class="line">    links = response.xpath(&apos;//td[descendant::a[contains(@href, &quot;#pagerank&quot;)]]/following-sibling::td/font&apos;)</span><br><span class="line"></span><br><span class="line">    for link in links:</span><br><span class="line">        item = DirectoryItem()</span><br><span class="line">        item[&apos;name&apos;] = link.xpath(&apos;a/text()&apos;).extract()</span><br><span class="line">        item[&apos;url&apos;] = link.xpath(&apos;a/@href&apos;).extract()</span><br><span class="line">        item[&apos;description&apos;] = link.xpath(&apos;font[2]/text()&apos;).extract()</span><br><span class="line">        yield item</span><br></pre></td></tr></table></figure>
<p>注意，您可能会遇到有些在Firebug找到，但是在原始HTML中找不到的元素， 例如典型的 <code>&lt;tbody&gt;</code> 元素， 或者Firebug检查活动DOM(live DOM)所看到的元素，但元素由javascript动态生成，并不在HTML源码中。 (原文语句乱了,上面为意译- -: or tags which Therefer in page HTML sources may on Firebug inspects the live DOM )</p>
<h1 id="调试内存溢出"><a href="#调试内存溢出" class="headerlink" title="调试内存溢出"></a>调试内存溢出</h1><p>在Scrapy中，类似Requests, Response及Items的对象具有有限的生命周期: 他们被创建，使用，最后被销毁。</p>
<p>这些对象中，Request的生命周期应该是最长的，其会在调度队列(Scheduler queue)中一直等待，直到被处理。 更多内容请参考 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/architecture.html#topics-architecture" target="_blank" rel="noopener">架构概览</a> 。</p>
<p>由于这些Scrapy对象拥有很长的生命，因此将这些对象存储在内存而没有正确释放的危险总是存在。 而这导致了所谓的”内存泄露”。</p>
<p>为了帮助调试内存泄露，Scrapy提供了跟踪对象引用的机制，叫做 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/leaks.html#topics-leaks-trackrefs" target="_blank" rel="noopener">trackref</a>， 或者您也可以使用第三方提供的更先进内存调试库 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/leaks.html#topics-leaks-guppy" target="_blank" rel="noopener">Guppy</a> (更多内容请查看下面)。而这都必须在 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/telnetconsole.html#topics-telnetconsole" target="_blank" rel="noopener">Telnet终端</a> 中使用。</p>
<h2 id="内存泄露的常见原因"><a href="#内存泄露的常见原因" class="headerlink" title="内存泄露的常见原因"></a>内存泄露的常见原因</h2><p>内存泄露经常是由于Scrapy开发者在Requests中(有意或无意)传递对象的引用(例如，使用 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/request-response.html#scrapy.http.Request.meta" target="_blank" rel="noopener"><code>meta</code></a> 属性或request回调函数)，使得该对象的生命周期与 Request的生命周期所绑定。这是目前为止最常见的内存泄露的原因， 同时对新手来说也是一个比较难调试的问题。</p>
<p>在大项目中，spider是由不同的人所编写的。而这其中有的spider可能是有”泄露的”， 当所有的爬虫同时运行时，这些影响了其他(写好)的爬虫，最终，影响了整个爬取进程。</p>
<p>如果您没有正确释放（以前分配的）资源，泄漏也可能来自您编写的自定义中间件，管道或扩展。例如，如果您<a href="https://doc.scrapy.org/en/1.5/topics/practices.html#run-multiple-spiders" target="_blank" rel="noopener">为每个进程</a>运行<a href="https://doc.scrapy.org/en/1.5/topics/practices.html#run-multiple-spiders" target="_blank" rel="noopener">多个蜘蛛</a>，分配资源<a href="https://doc.scrapy.org/en/1.5/topics/signals.html#std:signal-spider_opened" target="_blank" rel="noopener"><code>spider_opened</code></a> 但不释放它们<a href="https://doc.scrapy.org/en/1.5/topics/signals.html#std:signal-spider_closed" target="_blank" rel="noopener"><code>spider_closed</code></a>可能会导致问题。</p>
<h3 id="请求过多？"><a href="#请求过多？" class="headerlink" title="请求过多？"></a>请求过多？</h3><p>默认情况下，Scrapy将请求队列保存在内存中; 它包含 <a href="https://doc.scrapy.org/en/1.5/topics/request-response.html#scrapy.http.Request" target="_blank" rel="noopener"><code>Request</code></a>对象和Request属性中引用的所有对象（例如in <a href="https://doc.scrapy.org/en/1.5/topics/request-response.html#scrapy.http.Request.meta" target="_blank" rel="noopener"><code>meta</code></a>）。虽然不一定是泄漏，但这可能需要很多内存。启用 <a href="https://doc.scrapy.org/en/1.5/topics/jobs.html#topics-jobs" target="_blank" rel="noopener">持久作业队列</a>可以帮助控制内存使用情况。</p>
<h2 id="使用-trackref-调试内存泄露"><a href="#使用-trackref-调试内存泄露" class="headerlink" title="使用 trackref 调试内存泄露"></a>使用 <code>trackref</code> 调试内存泄露</h2><p><code>trackref</code> 是Scrapy提供用于调试大部分内存泄露情况的模块。 简单来说，其追踪了所有活动(live)的Request, Request, Item及Selector对象的引用。</p>
<p>您可以进入telnet终端并通过 <code>prefs()</code> 功能来检查多少(上面所提到的)活跃(alive)对象。 <code>pref()</code> 是 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/leaks.html#scrapy.utils.trackref.print_live_refs" target="_blank" rel="noopener"><code>print_live_refs()</code></a> 功能的引用:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">telnet localhost 6023</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; prefs()</span><br><span class="line">Live References</span><br><span class="line"></span><br><span class="line">ExampleSpider                       1   oldest: 15s ago</span><br><span class="line">HtmlResponse                       10   oldest: 1s ago</span><br><span class="line">Selector                            2   oldest: 0s ago</span><br><span class="line">FormRequest                       878   oldest: 7s ago</span><br></pre></td></tr></table></figure>
<p>正如所见，报告也展现了每个类中最老的对象的时间(age)。</p>
<p>如果您有内存泄露，那您能找到哪个spider正在泄露的机会是查看最老的request或response。 您可以使用 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/leaks.html#scrapy.utils.trackref.get_oldest" target="_blank" rel="noopener"><code>get_oldest()</code></a> 方法来获取每个类中最老的对象， 正如此所示(在终端中)(原文档没有样例)。</p>
<h3 id="哪些对象被追踪了"><a href="#哪些对象被追踪了" class="headerlink" title="哪些对象被追踪了?"></a>哪些对象被追踪了?</h3><p><code>trackref</code> 追踪的对象包括以下类(及其子类)的对象:</p>
<ul>
<li><code>scrapy.http.Request</code></li>
<li><code>scrapy.http.Response</code></li>
<li><code>scrapy.item.Item</code></li>
<li><code>scrapy.selector.Selector</code></li>
<li><code>scrapy.spider.Spider</code></li>
</ul>
<h3 id="真实例子"><a href="#真实例子" class="headerlink" title="真实例子"></a>真实例子</h3><p>让我们来看一个假设的具有内存泄露的准确例子。</p>
<p>假如我们有些spider的代码中有一行类似于这样的代码:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">return Request(&quot;http://www.somenastyspider.com/product.php?pid=%d&quot; % product_id,</span><br><span class="line">    callback=self.parse, meta=&#123;referer: response&#125;)</span><br></pre></td></tr></table></figure>
<p>代码中在request中传递了一个response的引用，使得reponse的生命周期与request所绑定， 进而造成了内存泄露。</p>
<p>让我们来看看如何使用 <code>trackref</code> 工具来发现哪一个是有问题的spider(当然是在不知道任何的前提的情况下)。</p>
<p>当crawler运行了一小阵子后，我们发现内存占用增长了很多。 这时候我们进入telnet终端，查看活跃(live)的引用:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; prefs()</span><br><span class="line">Live References</span><br><span class="line"></span><br><span class="line">SomenastySpider                     1   oldest: 15s ago</span><br><span class="line">HtmlResponse                     3890   oldest: 265s ago</span><br><span class="line">Selector                            2   oldest: 0s ago</span><br><span class="line">Request                          3878   oldest: 250s ago</span><br></pre></td></tr></table></figure>
<p>事实上有很多现场responses（并且它们太旧了）肯定是可疑的，因为与responses相比，Requests应该具有相对较短的生命周期。responses的数量与Requests的数量相似，因此看起来它们以某种方式相关联。我们现在可以去检查蜘蛛的代码来发现产生泄漏的令人讨厌的线（在请求中传递响应引用）。</p>
<p>有时候关于活动对象的额外信息会有所帮助。让我们来检查最老的回应：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from scrapy.utils.trackref import get_oldest</span><br><span class="line">&gt;&gt;&gt; r = get_oldest(&apos;HtmlResponse&apos;)</span><br><span class="line">&gt;&gt;&gt; r.url</span><br><span class="line">&apos;http://www.somenastyspider.com/product.php?pid=123&apos;</span><br></pre></td></tr></table></figure>
<p>如果你想遍历所有的对象，而不是获取最老的对象，你可以使用这个<a href="https://doc.scrapy.org/en/1.5/topics/leaks.html#scrapy.utils.trackref.iter_all" target="_blank" rel="noopener"><code>scrapy.utils.trackref.iter_all()</code></a>函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from scrapy.utils.trackref import iter_all</span><br><span class="line">&gt;&gt;&gt; [r.url for r in iter_all(&apos;HtmlResponse&apos;)]</span><br><span class="line">[&apos;http://www.somenastyspider.com/product.php?pid=123&apos;,</span><br><span class="line"> &apos;http://www.somenastyspider.com/product.php?pid=584&apos;,</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="很多spider"><a href="#很多spider" class="headerlink" title="很多spider?"></a>很多spider?</h3><p>如果您的项目有很多的spider，<code>prefs()</code> 的输出会变得很难阅读。针对于此， 该方法具有 <code>ignore</code> 参数，用于忽略特定的类(及其子类)。例如，这不会显示任何对蜘蛛的实时引用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from scrapy.spiders import Spider</span><br><span class="line">&gt;&gt;&gt; prefs(ignore=Spider)</span><br></pre></td></tr></table></figure>
<h3 id="scrapy-utils-trackref模块"><a href="#scrapy-utils-trackref模块" class="headerlink" title="scrapy.utils.trackref模块"></a>scrapy.utils.trackref模块</h3><p>以下是 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/leaks.html#module-scrapy.utils.trackref" target="_blank" rel="noopener"><code>trackref</code></a> 模块中可用的方法。</p>
<ul>
<li><p><em>class</em><code>scrapy.utils.trackref.object_ref</code></p>
<p>如果您想通过 <code>trackref</code> 模块追踪活跃的实例，继承该类(而不是对象)。</p>
</li>
</ul>
<ul>
<li><p><code>scrapy.utils.trackref.print_live_refs</code>(<em>class_name</em>, <em>ignore=NoneType</em>)</p>
<p>打印活跃引用的报告，以类名分类。参数:<strong>ignore</strong> (<em>类或者类的元组</em>) – 如果给定，所有指定类(或者类的元组)的对象将会被忽略。</p>
</li>
</ul>
<ul>
<li><p><code>scrapy.utils.trackref.get_oldest</code>(<em>class_name</em>)</p>
<p>返回给定类名的最老活跃(alive)对象，如果没有则返回 <code>None</code> 。首先使用<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/leaks.html#scrapy.utils.trackref.print_live_refs" target="_blank" rel="noopener"><code>print_live_refs()</code></a> 来获取每个类所跟踪的所有活跃(live)对象的列表。</p>
</li>
</ul>
<ul>
<li><p><code>scrapy.utils.trackref.iter_all</code>(<em>class_name</em>)</p>
<p>返回一个能给定类名的所有活跃对象的迭代器，如果没有则返回 <code>None</code>。首先使用 <a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/leaks.html#scrapy.utils.trackref.print_live_refs" target="_blank" rel="noopener"><code>print_live_refs()</code></a> 来获取每个类所跟踪的所有活跃(live)对象的列表。</p>
</li>
</ul>
<h2 id="使用Guppy调试内存泄露"><a href="#使用Guppy调试内存泄露" class="headerlink" title="使用Guppy调试内存泄露"></a>使用Guppy调试内存泄露</h2><p><code>trackref</code> 提供了追踪内存泄露非常方便的机制，其仅仅追踪了比较可能导致内存泄露的对象 (Requests, Response, Items及Selectors)。然而，内存泄露也有可能来自其他(更为隐蔽的)对象。 如果是因为这个原因，通过 <code>trackref</code> 则无法找到泄露点，您仍然有其他工具: <a href="http://pypi.python.org/pypi/guppy" target="_blank" rel="noopener">Guppy library</a> 。</p>
<p>如果使用 <code>setuptools</code> , 您可以通过下列命令安装Guppy:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">easy_install guppy</span><br></pre></td></tr></table></figure>
<p>telnet终端也提供了快捷方式(<code>hpy</code>)来访问Guppy堆对象(heap objects)。 下面给出了查看堆中所有可用的Python对象的例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = hpy.heap()</span><br><span class="line">&gt;&gt;&gt; x.bytype</span><br><span class="line">Partition of a set of 297033 objects. Total size = 52587824 bytes.</span><br><span class="line"> Index  Count   %     Size   % Cumulative  % Type</span><br><span class="line">     0  22307   8 16423880  31  16423880  31 dict</span><br><span class="line">     1 122285  41 12441544  24  28865424  55 str</span><br><span class="line">     2  68346  23  5966696  11  34832120  66 tuple</span><br><span class="line">     3    227   0  5836528  11  40668648  77 unicode</span><br><span class="line">     4   2461   1  2222272   4  42890920  82 type</span><br><span class="line">     5  16870   6  2024400   4  44915320  85 function</span><br><span class="line">     6  13949   5  1673880   3  46589200  89 types.CodeType</span><br><span class="line">     7  13422   5  1653104   3  48242304  92 list</span><br><span class="line">     8   3735   1  1173680   2  49415984  94 _sre.SRE_Pattern</span><br><span class="line">     9   1209   0   456936   1  49872920  95 scrapy.http.headers.Headers</span><br><span class="line">&lt;1676 more rows. Type e.g. &apos;_.more&apos; to view.&gt;</span><br></pre></td></tr></table></figure>
<p>您可以看到大部分的空间被字典所使用。接着，如果您想要查看哪些属性引用了这些字典， 您可以:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x.bytype[0].byvia</span><br><span class="line">Partition of a set of 22307 objects. Total size = 16423880 bytes.</span><br><span class="line"> Index  Count   %     Size   % Cumulative  % Referred Via:</span><br><span class="line">     0  10982  49  9416336  57   9416336  57 &apos;.__dict__&apos;</span><br><span class="line">     1   1820   8  2681504  16  12097840  74 &apos;.__dict__&apos;, &apos;.func_globals&apos;</span><br><span class="line">     2   3097  14  1122904   7  13220744  80</span><br><span class="line">     3    990   4   277200   2  13497944  82 &quot;[&apos;cookies&apos;]&quot;</span><br><span class="line">     4    987   4   276360   2  13774304  84 &quot;[&apos;cache&apos;]&quot;</span><br><span class="line">     5    985   4   275800   2  14050104  86 &quot;[&apos;meta&apos;]&quot;</span><br><span class="line">     6    897   4   251160   2  14301264  87 &apos;[2]&apos;</span><br><span class="line">     7      1   0   196888   1  14498152  88 &quot;[&apos;moduleDict&apos;]&quot;, &quot;[&apos;modules&apos;]&quot;</span><br><span class="line">     8    672   3   188160   1  14686312  89 &quot;[&apos;cb_kwargs&apos;]&quot;</span><br><span class="line">     9     27   0   155016   1  14841328  90 &apos;[1]&apos;</span><br><span class="line">&lt;333 more rows. Type e.g. &apos;_.more&apos; to view.&gt;</span><br></pre></td></tr></table></figure>
<p>如上所示，Guppy模块十分强大，不过也需要一些关于Python内部的知识。关于Guppy的更多内容请参考 <a href="http://guppy-pe.sourceforge.net/" target="_blank" rel="noopener">Guppy documentation</a>.</p>
<h2 id="Leaks-without-leaks"><a href="#Leaks-without-leaks" class="headerlink" title="Leaks without leaks"></a>Leaks without leaks</h2><p>有时候，您可能会注意到Scrapy进程的内存占用只在增长，从不下降。不幸的是， 有时候这并不是Scrapy或者您的项目在泄露内存。这是由于一个已知(但不有名)的Python问题。 Python在某些情况下可能不会返回已经释放的内存到操作系统。关于这个问题的更多内容请看:</p>
<ul>
<li><a href="http://evanjones.ca/python-memory.html" target="_blank" rel="noopener">Python Memory Management</a></li>
<li><a href="http://evanjones.ca/python-memory-part2.html" target="_blank" rel="noopener">Python Memory Management Part 2</a></li>
<li><a href="http://evanjones.ca/python-memory-part3.html" target="_blank" rel="noopener">Python Memory Management Part 3</a></li>
</ul>
<p>改进方案由Evan Jones提出，在 <a href="http://evanjones.ca/memoryallocator/" target="_blank" rel="noopener">这篇文章</a> 中详细介绍，在Python 2.5中合并。 不过这仅仅减小了这个问题，并没有完全修复。引用这片文章:</p>
<blockquote>
<p><em>不幸的是，这个patch仅仅会释放没有在其内部分配对象的区域(arena)。这意味着 碎片化是一个大问题。某个应用可以拥有很多空闲内存，分布在所有的区域(arena)中， 但是没法释放任何一个。这个问题存在于所有内存分配器中。解决这个问题的唯一办法是 转化到一个更为紧凑(compact)的垃圾回收器，其能在内存中移动对象。 这需要对Python解析器做一个显著的修改。</em></p>
</blockquote>
<p>这个问题将会在未来Scrapy发布版本中得到解决。我们打算转化到一个新的进程模型， 并在可回收的子进程池中运行spider。</p>
<h1 id="下载和处理文件和图像"><a href="#下载和处理文件和图像" class="headerlink" title="下载和处理文件和图像"></a>下载和处理文件和图像</h1><p>Scrapy提供可重复使用的<a href="https://doc.scrapy.org/en/latest/topics/item-pipeline.html" target="_blank" rel="noopener">项目管道，</a>用于下载附加到特定项目的文件（例如，当您刮擦产品并且还想在本地下载其图像时）。这些管道共享一些功能和结构（我们将它们称为介质管道），但通常您可以使用“文件管道”或“图像管道”。</p>
<p>两个管道都实现这些功能：</p>
<ul>
<li>避免重新下载最近下载的媒体</li>
<li>指定存储介质的位置（文件系统目录，Amazon S3存储桶，Google云存储存储桶）</li>
</ul>
<p>图像管道有几个额外的功能来处理图像：</p>
<ul>
<li>将所有下载的图像转换为通用格式（JPG）和模式（RGB）</li>
<li>生成缩略图</li>
<li>检查图像宽度/高度以确保它们符合最小限制</li>
</ul>
<p>管道还保留当前正在计划下载的那些媒体URL的内部队列，并将包含相同媒体到达的那些响应连接到该队列。这避免了多个项目共享多次下载相同的媒体。</p>
<h2 id="使用文件管道"><a href="#使用文件管道" class="headerlink" title="使用文件管道"></a>使用文件管道</h2><p>典型的工作流程<code>FilesPipeline</code>如下所示：</p>
<ol>
<li>在一个爬虫里，您抓取一个项目并将所需的URL放入一个 <code>file_urls</code>字段中。</li>
<li>该项目从爬虫内返回并转到物品管道。</li>
<li>当该项目进入<code>FilesPipeline</code>，该<code>file_urls</code>字段中的URL 将使用标准的Scrapy调度器和下载器（这意味着调度器和下载器中间件可以重复使用）计划下载，但具有更高的优先级，会在其他页面被抓取前处理。该项目在该特定管道阶段保持“锁定(locker)”状态，直到文件完成下载（或由于某种原因未完成下载）。</li>
<li>下载文件时，另一个字段（<code>files</code>）将被更新到结构中。该字段将包含一个含有下载文件信息的字典列表，例如下载的路径，原始抓取的URL（从<code>file_urls</code>字段获取）和文件校验码。该<code>files</code>字段列表中的文件将保留原始<code>file_urls</code>字段的相同顺序。如果某个文件下载失败，则会记录下错误信息并且该文件不会出现在该<code>files</code>字段中。</li>
</ol>
<h2 id="使用图像管道"><a href="#使用图像管道" class="headerlink" title="使用图像管道"></a>使用图像管道</h2><p>使用<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline" target="_blank" rel="noopener"><code>ImagesPipeline</code></a>很像使用<code>FilesPipeline</code>，除了使用的默认字段名称不同：您的<code>image_urls</code>用于项目的图像URL，它将填充<code>images</code>字段以获取有关下载图像的信息。</p>
<p>对图像文件使用<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline" target="_blank" rel="noopener"><code>ImagesPipeline</code></a>的优点是，您可以配置一些额外的功能，例如生成缩略图和根据图像大小过滤图像。</p>
<p>Images Pipeline使用<a href="https://github.com/python-pillow/Pillow" target="_blank" rel="noopener">Pillow</a>将图像缩略图和规格化为JPEG / RGB格式，因此您需要安装此库才能使用它。 在大多数情况下，<a href="http://www.pythonware.com/products/pil/" target="_blank" rel="noopener">Python成像库</a>（PIL）也应该可以工作，但是在某些设置中会导致麻烦，所以我们建议使用<a href="https://github.com/python-pillow/Pillow" target="_blank" rel="noopener">Pillow</a>而不是PIL。</p>
<h2 id="启用媒体管道"><a href="#启用媒体管道" class="headerlink" title="启用媒体管道"></a>启用媒体管道</h2><p>要启用媒体管道，您必须先将其添加到您的项目 <a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-ITEM_PIPELINES" target="_blank" rel="noopener"><code>ITEM_PIPELINES</code></a>设置中。</p>
<p>对于图像管道，请使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;&apos;scrapy.pipelines.images.ImagesPipeline&apos;: 1&#125;</span><br></pre></td></tr></table></figure>
<p>对于文件管道，请使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;&apos;scrapy.pipelines.files.FilesPipeline&apos;: 1&#125;</span><br></pre></td></tr></table></figure>
<p>注意</p>
<p>您也可以同时使用文件和图像管道。</p>
<p>然后，将目标存储设置配置为用于存储下载图像的有效值。否则，管道将保持禁用状态，即使您将其包含在<a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-ITEM_PIPELINES" target="_blank" rel="noopener"><code>ITEM_PIPELINES</code></a>设置中。</p>
<p>对于文件管道，请设置以下<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-FILES_STORE" target="_blank" rel="noopener"><code>FILES_STORE</code></a>设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FILES_STORE = &apos;/path/to/valid/dir&apos;</span><br></pre></td></tr></table></figure>
<p>对于图像管线，设置<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_STORE" target="_blank" rel="noopener"><code>IMAGES_STORE</code></a>设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_STORE = &apos;/path/to/valid/dir&apos;</span><br></pre></td></tr></table></figure>
<h2 id="支持的存储"><a href="#支持的存储" class="headerlink" title="支持的存储"></a>支持的存储</h2><p>文件系统目前是唯一官方支持的存储，但也支持在<a href="https://aws.amazon.com/s3/" target="_blank" rel="noopener">Amazon S3</a>和<a href="https://cloud.google.com/storage/" target="_blank" rel="noopener">Google云存储中</a>存储文件。</p>
<h3 id="文件系统存储"><a href="#文件系统存储" class="headerlink" title="文件系统存储"></a>文件系统存储</h3><p>这些文件使用它们的URL 的<a href="https://en.wikipedia.org/wiki/SHA_hash_functions" target="_blank" rel="noopener">SHA1散列</a>来存储文件名。</p>
<p>例如，以下图片网址：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.example.com/image.jpg</span><br></pre></td></tr></table></figure>
<p>谁的SHA1hash是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3afec3b4765f8f0a07b78f98c07b83f013567a0a</span><br></pre></td></tr></table></figure>
<p>将被下载并存储在以下文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;IMAGES_STORE&gt;/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg</span><br></pre></td></tr></table></figure>
<p>哪里：</p>
<ul>
<li><code>&lt;IMAGES_STORE&gt;</code>是<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_STORE" target="_blank" rel="noopener"><code>IMAGES_STORE</code></a>图像管道设置中定义的目录。</li>
<li><code>full</code>是将完整图像与缩略图分开的子目录（如果使用的话）。有关更多信息，请参阅<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#topics-images-thumbnails" target="_blank" rel="noopener">图像的缩略图生成</a>。</li>
</ul>
<h3 id="Amazon-S3存储"><a href="#Amazon-S3存储" class="headerlink" title="Amazon S3存储"></a>Amazon S3存储</h3><p><a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-FILES_STORE" target="_blank" rel="noopener"><code>FILES_STORE</code></a>和<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_STORE" target="_blank" rel="noopener"><code>IMAGES_STORE</code></a>可以代表Amazon S3存储桶。Scrapy会自动将文件上传到存储桶。</p>
<p>例如，这是一个有效的<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_STORE" target="_blank" rel="noopener"><code>IMAGES_STORE</code></a>值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_STORE = &apos;s3://bucket/images&apos;</span><br></pre></td></tr></table></figure>
<p>您可以修改用于存储文件的访问控制列表（ACL）策略，这是由<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-FILES_STORE_S3_ACL" target="_blank" rel="noopener"><code>FILES_STORE_S3_ACL</code></a>和 <a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_STORE_S3_ACL" target="_blank" rel="noopener"><code>IMAGES_STORE_S3_ACL</code></a>设置定义的。默认情况下，ACL被设置为 <code>private</code>。要使文件公开可用，请使用以下<code>public-read</code> 策略：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_STORE_S3_ACL = &apos;public-read&apos;</span><br></pre></td></tr></table></figure>
<p>有关更多信息，请参阅Amazon S3开发人员指南中的<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl" target="_blank" rel="noopener">预装ACL</a>。</p>
<h3 id="Google云端存储"><a href="#Google云端存储" class="headerlink" title="Google云端存储"></a>Google云端存储</h3><p><a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-FILES_STORE" target="_blank" rel="noopener"><code>FILES_STORE</code></a>和<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_STORE" target="_blank" rel="noopener"><code>IMAGES_STORE</code></a>可以代表Google云存储存储分区。Scrapy会自动将文件上传到存储桶。（需要<a href="https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python" target="_blank" rel="noopener">谷歌云存储</a>）</p>
<p>例如，这些是有效的<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_STORE" target="_blank" rel="noopener"><code>IMAGES_STORE</code></a>和<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-GCS_PROJECT_ID" target="_blank" rel="noopener"><code>GCS_PROJECT_ID</code></a>设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_STORE = &apos;gs://bucket/images/&apos;</span><br><span class="line">GCS_PROJECT_ID = &apos;project_id&apos;</span><br></pre></td></tr></table></figure>
<p>有关身份验证的信息，请参阅此<a href="https://cloud.google.com/docs/authentication/production" target="_blank" rel="noopener">文档</a>。</p>
<p>为了首先使用媒体管道，<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#topics-media-pipeline-enabling" target="_blank" rel="noopener">启用它</a>。</p>
<p>然后，如果spider用URLs键（<code>file_urls</code>或者 <code>image_urls</code>分别为文件或图像管线）返回字典，管道将把结果放在相应的键（<code>files</code>或<code>images</code>）下。</p>
<p>如果您更喜欢使用<a href="https://doc.scrapy.org/en/latest/topics/items.html#scrapy.item.Item" target="_blank" rel="noopener"><code>Item</code></a>，那么使用必要的字段定义一个自定义项目，例如图像管道的示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class MyItem(scrapy.Item):</span><br><span class="line"></span><br><span class="line">    # ... other item fields ...</span><br><span class="line">    image_urls = scrapy.Field()</span><br><span class="line">    images = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>如果要为URL键或结果键使用另一个字段名称，也可以覆盖它。</p>
<p>对于文件管道，设置<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-FILES_URLS_FIELD" target="_blank" rel="noopener"><code>FILES_URLS_FIELD</code></a>和/或 <a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-FILES_RESULT_FIELD" target="_blank" rel="noopener"><code>FILES_RESULT_FIELD</code></a>设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FILES_URLS_FIELD = &apos;field_name_for_your_files_urls&apos;</span><br><span class="line">FILES_RESULT_FIELD = &apos;field_name_for_your_processed_files&apos;</span><br></pre></td></tr></table></figure>
<p>对于图像管线，设置<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_URLS_FIELD" target="_blank" rel="noopener"><code>IMAGES_URLS_FIELD</code></a>和/或 <a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_RESULT_FIELD" target="_blank" rel="noopener"><code>IMAGES_RESULT_FIELD</code></a>设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_URLS_FIELD = &apos;field_name_for_your_images_urls&apos;</span><br><span class="line">IMAGES_RESULT_FIELD = &apos;field_name_for_your_processed_images&apos;</span><br></pre></td></tr></table></figure>
<p>如果您需要更复杂的内容并希望覆盖自定义管道行为，请参阅<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#topics-media-pipeline-override" target="_blank" rel="noopener">扩展介质管道</a>。</p>
<p>如果您有多个图像管道从ImagePipeline继承，并且您希望在不同管道中具有不同的设置，则可以设置以管道类的大写名称开头的设置键。例如，如果您的管道被称为MyPipeline，并且您想定制IMAGES_URLS_FIELD，则可以定义设置MYPIPELINE_IMAGES_URLS_FIELD并使用您的自定义设置。</p>
<h2 id="附加功能"><a href="#附加功能" class="headerlink" title="附加功能"></a>附加功能</h2><h3 id="文件到期"><a href="#文件到期" class="headerlink" title="文件到期"></a>文件到期</h3><p>图像管道避免了下载最近下载的文件。要调整此保留延迟，请使用<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-FILES_EXPIRES" target="_blank" rel="noopener"><code>FILES_EXPIRES</code></a>设置（或者 <a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_EXPIRES" target="_blank" rel="noopener"><code>IMAGES_EXPIRES</code></a>在图像管道的情况下），该设置指定延迟天数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 120 days of delay for files expiration</span><br><span class="line">FILES_EXPIRES = 120</span><br><span class="line"></span><br><span class="line"># 30 days of delay for images expiration</span><br><span class="line">IMAGES_EXPIRES = 30</span><br></pre></td></tr></table></figure>
<p>这两个设置的默认值是90天。</p>
<p>如果你有管道的子类FilesPipeline，你想有不同的设置，你可以设置以大写的类名开头的设置键。例如，给定管道类名为MyPipeline，您可以设置设置键：</p>
<p>MYPIPELINE_FILES_EXPIRES = 180</p>
<p>并且管道类MyPipeline将到期时间设置为180。</p>
<h3 id="为图像生成缩略图"><a href="#为图像生成缩略图" class="headerlink" title="为图像生成缩略图"></a>为图像生成缩略图</h3><p>图像管道可以自动创建下载图像的缩略图。</p>
<p>为了使用此功能，您必须设置<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_THUMBS" target="_blank" rel="noopener"><code>IMAGES_THUMBS</code></a>一个字典，其中的键是缩略图名称，值是它们的尺寸。</p>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_THUMBS = &#123;</span><br><span class="line">    &apos;small&apos;: (50, 50),</span><br><span class="line">    &apos;big&apos;: (270, 270),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当您使用此功能时，图像管道将使用以下格式创建每个指定尺寸的缩略图：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;IMAGES_STORE&gt;/thumbs/&lt;size_name&gt;/&lt;image_id&gt;.jpg</span><br></pre></td></tr></table></figure>
<p>哪里：</p>
<ul>
<li><code>&lt;size_name&gt;</code>是在指定的<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_THUMBS" target="_blank" rel="noopener"><code>IMAGES_THUMBS</code></a> 字典键（<code>small</code>，<code>big</code>等）</li>
<li><code>&lt;image_id&gt;</code>是图片网址的<a href="https://en.wikipedia.org/wiki/SHA_hash_functions" target="_blank" rel="noopener">SHA1哈希值</a></li>
</ul>
<p>使用<code>small</code>和<code>big</code>缩略图名称存储的图像文件示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;IMAGES_STORE&gt;/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg</span><br><span class="line">&lt;IMAGES_STORE&gt;/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg</span><br><span class="line">&lt;IMAGES_STORE&gt;/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg</span><br></pre></td></tr></table></figure>
<p>第一个是从网站下载的完整图像。</p>
<h3 id="过滤掉小图片"><a href="#过滤掉小图片" class="headerlink" title="过滤掉小图片"></a>过滤掉小图片</h3><p>使用图像管线时，可以通过指定<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_MIN_HEIGHT" target="_blank" rel="noopener"><code>IMAGES_MIN_HEIGHT</code></a>和 <a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-IMAGES_MIN_WIDTH" target="_blank" rel="noopener"><code>IMAGES_MIN_WIDTH</code></a>设置中允许的最小尺寸来放置太小的图像。</p>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_MIN_HEIGHT = 110 </span><br><span class="line">IMAGES_MIN_WIDTH = 110</span><br></pre></td></tr></table></figure>
<p>注意</p>
<p>大小限制完全不影响缩略图生成。</p>
<p>可以只设置一个尺寸约束或者两者兼有。设置它们时，只会保存同时满足最小尺寸的图像。对于上述示例，大小（105 x 105）或（105 x 200）或（200 x 105）的图像将全部被删除，因为至少一个维度比约束更短。</p>
<p>默认情况下，没有大小限制，因此所有图像都被处理。</p>
<h3 id="允许重定向"><a href="#允许重定向" class="headerlink" title="允许重定向"></a>允许重定向</h3><p>默认情况下，媒体管道忽略重定向，即对媒体文件URL请求的HTTP重定向意味着媒体下载被认为失败。</p>
<p>要处理媒体重定向，请将此设置设置为<code>True</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MEDIA_ALLOW_REDIRECTS = True</span><br></pre></td></tr></table></figure>
<h2 id="扩展媒体管道"><a href="#扩展媒体管道" class="headerlink" title="扩展媒体管道"></a>扩展媒体管道</h2><p>在这里看到你可以在自定义文件管道中覆盖的方法：</p>
<p><em>class</em><code>scrapy.pipelines.files.`</code>FilesPipeline`</p>
<ul>
<li><p><code>get_media_requests</code>(<em>item</em>, <em>info</em>)</p>
<p>如工作流程所示，管道将获取要从项目下载的图像的URL。为了做到这一点，您可以覆盖该 <a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.get_media_requests" target="_blank" rel="noopener"><code>get_media_requests()</code></a>方法并为每个文件URL返回一个请求：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def get_media_requests(self, item, info):</span><br><span class="line">    for file_url in item[&apos;file_urls&apos;]:</span><br><span class="line">        yield scrapy.Request(file_url)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这些请求将由管道处理，并且当它们完成下载时，结果将会作为2-element元祖的列表发送到<a href="https://doc.scrapy.org/en/1.5/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.item_completed" target="_blank" rel="noopener"><code>item_completed()</code></a>。每个元组将包含以下内容：<code>(success, file_info_or_error)</code></p>
<ul>
<li><code>success</code>是一个布尔值，如果图像下载成功则返回<code>True</code> 由于某种原因失败了返回<code>False</code></li>
<li><code>file_info_or_error</code>是一个包含以下键（如果成功返回<code>Ture</code> ）的字典，或者如果出现了问题，则是 <a href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html" target="_blank" rel="noopener">Twisted Failure</a><ul>
<li><code>url</code> - 文件的下载地址。这是从<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.get_media_requests" target="_blank" rel="noopener"><code>get_media_requests()</code></a> 方法返回的请求的url 。</li>
<li><code>path</code>- <a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#std:setting-FILES_STORE" target="_blank" rel="noopener"><code>FILES_STORE</code></a>文件存储位置的路径（相对于）</li>
<li><code>checksum</code>- 图像内容的<a href="https://en.wikipedia.org/wiki/MD5" target="_blank" rel="noopener">MD5哈希</a></li>
</ul>
</li>
</ul>
<p>接收到的元组列表<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.item_completed" target="_blank" rel="noopener"><code>item_completed()</code></a>保证保持从该<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.get_media_requests" target="_blank" rel="noopener"><code>get_media_requests()</code></a>方法返回的请求的相同顺序 。</p>
<p>这是一个典型的<code>results</code>参数值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(True,</span><br><span class="line">  &#123;&apos;checksum&apos;: &apos;2b00042f7481c7b056c4b410d28f33cf&apos;,</span><br><span class="line">   &apos;path&apos;: &apos;full/7d97e98f8af710c7e7fe703abc8f639e0ee507c4.jpg&apos;,</span><br><span class="line">   &apos;url&apos;: &apos;http://www.example.com/images/product1.jpg&apos;&#125;),</span><br><span class="line"> (True,</span><br><span class="line">  &#123;&apos;checksum&apos;: &apos;b9628c4ab9b595f72f280b90c4fd093d&apos;,</span><br><span class="line">   &apos;path&apos;: &apos;full/1ca5879492b8fd606df1964ea3c1e2f4520f076f.jpg&apos;,</span><br><span class="line">   &apos;url&apos;: &apos;http://www.example.com/images/product2.jpg&apos;&#125;),</span><br><span class="line"> (False,</span><br><span class="line">  Failure(...))]</span><br></pre></td></tr></table></figure>
<p>默认情况下，该<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.get_media_requests" target="_blank" rel="noopener"><code>get_media_requests()</code></a>方法返回<code>None</code>，这意味着没有文件要下载的项目。</p>
<p><code>item_completed</code>(<em>results</em>, <em>items</em>, <em>info</em>)</p>
<p><a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.item_completed" target="_blank" rel="noopener"><code>FilesPipeline.item_completed()</code></a>当单个项目的所有文件请求都已完成（完成下载或由于某种原因失败）时调用此方法。</p>
<p>该<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.item_completed" target="_blank" rel="noopener"><code>item_completed()</code></a>方法必须返回将发送到后续项目管道阶段的输出，因此您必须返回（或丢弃）该项目，就像在任何管道中一样。</p>
<p>以下是<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.item_completed" target="_blank" rel="noopener"><code>item_completed()</code></a>我们将下载的文件路径（传递到结果中）存储在<code>file_paths</code> 项目字段中的方法示例，如果项目不包含任何文件，我们将删除该项目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line"></span><br><span class="line">def item_completed(self, results, item, info):</span><br><span class="line">    image_paths = [x[&apos;path&apos;] for ok, x in results if ok]</span><br><span class="line">    if not image_paths:</span><br><span class="line">        raise DropItem(&quot;Item contains no images&quot;)</span><br><span class="line">    item[&apos;image_paths&apos;] = image_paths</span><br><span class="line">    return item</span><br></pre></td></tr></table></figure>
<p>  默认情况下，该<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.item_completed" target="_blank" rel="noopener"><code>item_completed()</code></a>方法返回该项目。</p>
<p>在这里看到您可以在自定义图像管道中覆盖的方法：</p>
<p><em>class</em><code>scrapy.contrib.pipeline.images.ImagesPipeline</code></p>
<p>​    这<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline" target="_blank" rel="noopener"><code>ImagesPipeline</code></a>是<code>FilesPipeline</code>对字段名称进行自定义并为图像添加自定义行为的扩展。</p>
<p>​    <code>get_media_requests</code>（<em>item</em>，<em>info</em> ）</p>
<p>​        以与方法相同的方式工作<code>FilesPipeline.get_media_requests()</code>，但对图像网址使用不同的字段名称。</p>
<p>​        必须为每个图片网址返回一个请求。</p>
<p>​    <code>item_completed</code>(<em>results</em>, <em>item</em>, <em>info</em>)</p>
<p>​        <a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.item_completed" target="_blank" rel="noopener"><code>ImagesPipeline.item_completed()</code></a>当单个项目的所有图像请求都已完成（完成下载或由于某种原因失败）时调用此方法。</p>
<p>​        以与方法相同的方式工作<code>FilesPipeline.item_completed()</code>，但使用不同的字段名称来存储图像下载结果。</p>
<p>​        默认情况下，该<a href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.item_completed" target="_blank" rel="noopener"><code>item_completed()</code></a>方法返回该项目。</p>
<h2 id="自定义图像管道示例"><a href="#自定义图像管道示例" class="headerlink" title="自定义图像管道示例"></a>自定义图像管道示例</h2><p>下面是图像管道的完整示例，其示例方法如上所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.pipelines.images import ImagesPipeline</span><br><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line"></span><br><span class="line">class MyImagesPipeline(ImagesPipeline):</span><br><span class="line"></span><br><span class="line">    def get_media_requests(self, item, info):</span><br><span class="line">        for image_url in item[&apos;image_urls&apos;]:</span><br><span class="line">            yield scrapy.Request(image_url)</span><br><span class="line"></span><br><span class="line">    def item_completed(self, results, item, info):</span><br><span class="line">        image_paths = [x[&apos;path&apos;] for ok, x in results if ok]</span><br><span class="line">        if not image_paths:</span><br><span class="line">            raise DropItem(&quot;Item contains no images&quot;)</span><br><span class="line">        item[&apos;image_paths&apos;] = image_paths</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<h1 id="部署Spider"><a href="#部署Spider" class="headerlink" title="部署Spider"></a>部署Spider</h1><p>本节介绍您为部署Scrapy蜘蛛而定期运行它们的不同选项。在本地机器上运行Scrapy蜘蛛程序对于（早期）开发阶段来说非常方便，但是当您需要执行长时间运行的蜘蛛或移动蜘蛛来持续运行时，并不是那么重要。这是部署Scrapy蜘蛛解决方案的地方。</p>
<p>部署Scrapy蜘蛛的普遍选择是：</p>
<ul>
<li><a href="https://doc.scrapy.org/en/latest/topics/deploy.html#deploy-scrapyd" target="_blank" rel="noopener">Scrapyd</a>（开源）</li>
<li><a href="https://doc.scrapy.org/en/latest/topics/deploy.html#deploy-scrapy-cloud" target="_blank" rel="noopener">Scrapy云</a>（基于云）</li>
</ul>
<h2 id="部署到Scrapyd服务器"><a href="#部署到Scrapyd服务器" class="headerlink" title="部署到Scrapyd服务器"></a>部署到Scrapyd服务器</h2><p><a href="https://github.com/scrapy/scrapyd" target="_blank" rel="noopener">Scrapyd</a>是运行Scrapy蜘蛛的开源应用程序。它提供了一个HTTP API的服务器，能够运行和监控Scrapy蜘蛛。</p>
<p>要将Spider部署到Scrapyd，您可以使用由<a href="https://github.com/scrapy/scrapyd-client" target="_blank" rel="noopener">scrapyd-client</a>包提供的scrapyd-deploy工具。请参阅<a href="https://scrapyd.readthedocs.io/en/latest/deploy.html" target="_blank" rel="noopener">scrapyd-deploy文档</a>以获取更多信息。</p>
<p>Scrapyd由一些Scrapy开发人员维护。</p>
<h2 id="部署到Scrapy-Cloud"><a href="#部署到Scrapy-Cloud" class="headerlink" title="部署到Scrapy Cloud"></a>部署到Scrapy Cloud</h2><p><a href="https://scrapinghub.com/scrapy-cloud" target="_blank" rel="noopener">Scrapy Cloud</a>是<a href="https://scrapinghub.com/scrapy-cloud" target="_blank" rel="noopener">Scrapy</a>背后的<a href="https://scrapinghub.com/" target="_blank" rel="noopener">Scrapinghub</a>托管的基于云的服务。</p>
<p>Scrapy Cloud不需要安装和监控服务器，并提供了一个很好的用户界面来管理蜘蛛并查看抓取的项目，日志和统计信息。</p>
<p>要将Scider部署到Scrapy Cloud，您可以使用<a href="https://doc.scrapinghub.com/shub.html" target="_blank" rel="noopener">shub</a>命令行工具。请参阅<a href="https://doc.scrapinghub.com/scrapy-cloud.html" target="_blank" rel="noopener">Scrapy Cloud文档</a>以获取更多信息。</p>
<p>Scrapy Cloud与Scrapyd兼容，并且可以根据需要在它们之间切换 - 从<code>scrapy.cfg</code>文件中读取配置<code>scrapyd-deploy</code>。</p>
<h1 id="自动限速-AutoThrottle-扩展"><a href="#自动限速-AutoThrottle-扩展" class="headerlink" title="自动限速(AutoThrottle)扩展"></a>自动限速(AutoThrottle)扩展</h1><p>该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。</p>
<h2 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h2><ol>
<li>更友好的对待网站，而不使用默认的下载延迟0。</li>
<li>自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。 用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。</li>
</ol>
<h2 id="怎么运行的"><a href="#怎么运行的" class="headerlink" title="怎么运行的"></a>怎么运行的</h2><p>AutoThrottle扩展可动态调整下载延迟，以使蜘蛛向<a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY" target="_blank" rel="noopener"><code>AUTOTHROTTLE_TARGET_CONCURRENCY</code></a>每个远程网站平均发送 并发请求。</p>
<p>它使用下载延迟来计算延迟。其主要思想是：如果一台服务器需要<code>latency</code>秒钟响应，客户端应该发送一个请求的每个<code>latency/N</code>秒，具有<code>N</code>并行处理的请求。</p>
<p>而不是调整延迟，可以设置一个小的固定下载延迟并对并发使用<a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_DOMAIN</code></a>或 <a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_IP</code></a>选项施加硬性限制 。它会提供类似的效果，但有一些重要的区别：</p>
<ul>
<li>由于下载延迟很小，偶尔会出现一些请求;</li>
<li>通常非200（错误）响应可以比常规响应更快地返回，因此，在服务器开始返回错误时，如果下载延迟较小，并发限制爬网程序将更快地向服务器发送请求。但这与爬虫应该做的事情相反 - 如果发生错误，减慢速度更有意义：这些错误可能是由高请求率造成的。</li>
</ul>
<p>AutoThrottle没有这些问题。</p>
<h2 id="节流算法"><a href="#节流算法" class="headerlink" title="节流算法"></a>节流算法</h2><p>AutoThrottle算法根据以下规则调整下载延迟：</p>
<ol>
<li>蜘蛛总是以下载延迟开始 <a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_START_DELAY" target="_blank" rel="noopener"><code>AUTOTHROTTLE_START_DELAY</code></a>;</li>
<li>当接收到响应时，目标下载延迟被计算为 其中响应的等待时间，并且是。<code>latency / N`</code>latency<code></code>N<code>[</code>AUTOTHROTTLE_TARGET_CONCURRENCY`](<a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY</a>)</li>
<li>下次请求的下载延迟设置为上次下载延迟的平均值和目标下载延迟;</li>
<li>不允许200个响应的延迟减少延迟;</li>
<li>下载延迟不能小于<a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-DOWNLOAD_DELAY" target="_blank" rel="noopener"><code>DOWNLOAD_DELAY</code></a>或大于<a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_MAX_DELAY" target="_blank" rel="noopener"><code>AUTOTHROTTLE_MAX_DELAY</code></a></li>
</ol>
<p>注意</p>
<p>AutoThrottle扩展支持标准Scrapy设置的并发性和延迟。这意味着它会尊重 <a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_DOMAIN</code></a>和 <a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_IP</code></a>选择，永远不会将下载延迟设置为低于<a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-DOWNLOAD_DELAY" target="_blank" rel="noopener"><code>DOWNLOAD_DELAY</code></a>。</p>
<p>在Scrapy中，下载延迟时间的测量是建立TCP连接和接收HTTP头之间的时间。</p>
<p>请注意，这些延迟在协作式多任务环境中很难准确测量，因为Scrapy可能正忙于处理Spider回调，并且无法参加下载。但是，这些延迟仍应该对Scrapy（最终是服务器）的繁忙程度进行合理估计，并且此扩展建立在此前提之上。</p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>用于控制AutoThrottle扩展的设置是：</p>
<ul>
<li><a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_ENABLED" target="_blank" rel="noopener"><code>AUTOTHROTTLE_ENABLED</code></a></li>
<li><a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_START_DELAY" target="_blank" rel="noopener"><code>AUTOTHROTTLE_START_DELAY</code></a></li>
<li><a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_MAX_DELAY" target="_blank" rel="noopener"><code>AUTOTHROTTLE_MAX_DELAY</code></a></li>
<li><a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY" target="_blank" rel="noopener"><code>AUTOTHROTTLE_TARGET_CONCURRENCY</code></a></li>
<li><a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_DEBUG" target="_blank" rel="noopener"><code>AUTOTHROTTLE_DEBUG</code></a></li>
<li><a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_DOMAIN</code></a></li>
<li><a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_IP</code></a></li>
<li><a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-DOWNLOAD_DELAY" target="_blank" rel="noopener"><code>DOWNLOAD_DELAY</code></a></li>
</ul>
<p>有关更多信息，请参阅<a href="https://doc.scrapy.org/en/latest/topics/autothrottle.html#autothrottle-algorithm" target="_blank" rel="noopener">它如何工作</a>。</p>
<h3 id="AUTOTHROTTLE-ENABLED"><a href="#AUTOTHROTTLE-ENABLED" class="headerlink" title="AUTOTHROTTLE_ENABLED"></a>AUTOTHROTTLE_ENABLED</h3><p>默认： <code>False</code></p>
<p>启用AutoThrottle扩展。</p>
<h3 id="AUTOTHROTTLE-START-DELAY"><a href="#AUTOTHROTTLE-START-DELAY" class="headerlink" title="AUTOTHROTTLE_START_DELAY"></a>AUTOTHROTTLE_START_DELAY</h3><p>默认： <code>5.0</code></p>
<p>最初的下载延迟（以秒为单位）。</p>
<h3 id="AUTOTHROTTLE-MAX-DELAY"><a href="#AUTOTHROTTLE-MAX-DELAY" class="headerlink" title="AUTOTHROTTLE_MAX_DELAY"></a>AUTOTHROTTLE_MAX_DELAY</h3><p>默认： <code>60.0</code></p>
<p>在高延迟情况下设置的最大下载延迟（以秒为单位）。</p>
<h3 id="AUTOTHROTTLE-TARGET-CONCURRENCY"><a href="#AUTOTHROTTLE-TARGET-CONCURRENCY" class="headerlink" title="AUTOTHROTTLE_TARGET_CONCURRENCY"></a>AUTOTHROTTLE_TARGET_CONCURRENCY</h3><p>版本1.1中的新功能</p>
<p>默认： <code>1.0</code></p>
<p>Scrapy应平行发送到远程网站的平均请求数量。</p>
<p>默认情况下，AutoThrottle调整延迟以向每个远程网站发送单个并发请求。将此选项设置为更高的值（例如<code>2.0</code>）以增加吞吐量和远程服务器的负载。较低的<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>值（例如<code>0.5</code>）会使抓取工具更加保守和礼貌。</p>
<p>请注意，<a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_DOMAIN</code></a> 和<a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_IP</code></a>在启用AutoThrottle扩展选项仍然遵守。这意味着如果 <code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>设置的值高于 <a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_DOMAIN</code></a>或者<a href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP" target="_blank" rel="noopener"><code>CONCURRENT_REQUESTS_PER_IP</code></a>，搜寻器将不会达到这个并发请求数。</p>
<p>在每个给定的时间点，Scrapy可以发送比或多或少的并发请求<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>; 它是爬虫尝试接近的建议值，而不是硬限制。</p>
<h3 id="AUTOTHROTTLE-DEBUG"><a href="#AUTOTHROTTLE-DEBUG" class="headerlink" title="AUTOTHROTTLE_DEBUG"></a>AUTOTHROTTLE_DEBUG</h3><p>默认： <code>False</code></p>
<p>启用AutoThrottle调试模式，该模式将显示收到的每个响应的统计数据，以便您可以看到如何实时调整调节参数。</p>
<h1 id="Benchmarking"><a href="#Benchmarking" class="headerlink" title="Benchmarking"></a>Benchmarking</h1><p>0.17 新版功能.</p>
<p>Scrapy提供了一个简单的性能测试工具。其创建了一个本地HTTP服务器，并以最大可能的速度进行爬取。 该测试性能工具目的是测试Scrapy在您的硬件上的效率，来获得一个基本的底线用于对比。 其使用了一个简单的spider，仅跟进链接，不做任何处理。</p>
<p>运行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy bench</span><br></pre></td></tr></table></figure>
<p>您能看到类似的输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">2016-12-16 21:18:48 [scrapy.utils.log] INFO: Scrapy 1.2.2 started (bot: quotesbot)</span><br><span class="line">2016-12-16 21:18:48 [scrapy.utils.log] INFO: Overridden settings: &#123;&apos;CLOSESPIDER_TIMEOUT&apos;: 10, &apos;ROBOTSTXT_OBEY&apos;: True, &apos;SPIDER_MODULES&apos;: [&apos;quotesbot.spiders&apos;], &apos;LOGSTATS_INTERVAL&apos;: 1, &apos;BOT_NAME&apos;: &apos;quotesbot&apos;, &apos;LOG_LEVEL&apos;: &apos;INFO&apos;, &apos;NEWSPIDER_MODULE&apos;: &apos;quotesbot.spiders&apos;&#125;</span><br><span class="line">2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[&apos;scrapy.extensions.closespider.CloseSpider&apos;,</span><br><span class="line"> &apos;scrapy.extensions.logstats.LogStats&apos;,</span><br><span class="line"> &apos;scrapy.extensions.telnet.TelnetConsole&apos;,</span><br><span class="line"> &apos;scrapy.extensions.corestats.CoreStats&apos;]</span><br><span class="line">2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">[&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;]</span><br><span class="line">2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">[&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;]</span><br><span class="line">2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line">2016-12-16 21:18:49 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2016-12-16 21:18:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:50 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 4200 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:51 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:52 [scrapy.extensions.logstats] INFO: Crawled 198 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:53 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:54 [scrapy.extensions.logstats] INFO: Crawled 302 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:55 [scrapy.extensions.logstats] INFO: Crawled 358 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:56 [scrapy.extensions.logstats] INFO: Crawled 406 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:57 [scrapy.extensions.logstats] INFO: Crawled 438 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:58 [scrapy.extensions.logstats] INFO: Crawled 470 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:18:59 [scrapy.core.engine] INFO: Closing spider (closespider_timeout)</span><br><span class="line">2016-12-16 21:18:59 [scrapy.extensions.logstats] INFO: Crawled 518 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:19:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;&apos;downloader/request_bytes&apos;: 229995,</span><br><span class="line"> &apos;downloader/request_count&apos;: 534,</span><br><span class="line"> &apos;downloader/request_method_count/GET&apos;: 534,</span><br><span class="line"> &apos;downloader/response_bytes&apos;: 1565504,</span><br><span class="line"> &apos;downloader/response_count&apos;: 534,</span><br><span class="line"> &apos;downloader/response_status_count/200&apos;: 534,</span><br><span class="line"> &apos;finish_reason&apos;: &apos;closespider_timeout&apos;,</span><br><span class="line"> &apos;finish_time&apos;: datetime.datetime(2016, 12, 16, 16, 19, 0, 647725),</span><br><span class="line"> &apos;log_count/INFO&apos;: 17,</span><br><span class="line"> &apos;request_depth_max&apos;: 19,</span><br><span class="line"> &apos;response_received_count&apos;: 534,</span><br><span class="line"> &apos;scheduler/dequeued&apos;: 533,</span><br><span class="line"> &apos;scheduler/dequeued/memory&apos;: 533,</span><br><span class="line"> &apos;scheduler/enqueued&apos;: 10661,</span><br><span class="line"> &apos;scheduler/enqueued/memory&apos;: 10661,</span><br><span class="line"> &apos;start_time&apos;: datetime.datetime(2016, 12, 16, 16, 18, 49, 799869)&#125;</span><br><span class="line">2016-12-16 21:19:00 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)</span><br></pre></td></tr></table></figure>
<p>这说明了您的Scrapy能以3900页面/分钟的速度爬取。注意，这是一个非常简单，仅跟进链接的spider。 任何您所编写的spider会做更多处理，从而减慢爬取的速度。 减慢的程度取决于spider做的处理以及其是如何被编写的。</p>
<p>未来会有更多的用例会被加入到性能测试套装中，以覆盖更多常见的情景。</p>

    </article>
    <!-- license  -->
    
        <div class="license-wrapper">
            <p>原文作者: <a href="http://yoursite.com">Hu3sky</a>
            <p>原文链接: <a href="http://yoursite.com/2018/04/18/Broad Crawls~Benchmarking/">http://yoursite.com/2018/04/18/Broad Crawls~Benchmarking/</a>
            <p>发表日期: <a href="http://yoursite.com/2018/04/18/Broad Crawls~Benchmarking/">April 18th 2018, 6:42:10 pm</a>
            <p>版权声明: 本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p>
        </div>
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href= "/2018/04/20/scrapy爬虫/" title= scrapy爬虫实例 >
                    <div class="nextTitle">scrapy爬虫实例</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2018/04/17/webug wp/" title= webug wp >
                    <div class="prevTitle">webug wp</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
<div id="container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
    var gitment = new Gitment({
        // id: "", // 可选。默认为 location.href
        owner: 'Hu3sky',
        repo: 'gitment-comments',
        oauth: {
            client_id: '7b315f10a637afb815a9',
            client_secret: 'f151efdf48db67e59ed518492bbd1ef12b680a00',
        },
    })
    gitment.render('container')

</script>

    <!--PC和WAP自适应版-->

    <!--PC版-->


    
    

    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto://hu3sky@d0g3.cn" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/Hu3sky" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#通用爬虫-Broad-Crawls"><span class="toc-number">1.</span> <span class="toc-text">通用爬虫(Broad Crawls)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#增加并发"><span class="toc-number">1.1.</span> <span class="toc-text">增加并发</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#增加Twisted-IO线程池的最大大小"><span class="toc-number">1.2.</span> <span class="toc-text">增加Twisted IO线程池的最大大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#设置你自己的DNS"><span class="toc-number">1.3.</span> <span class="toc-text">设置你自己的DNS</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#降低log级别"><span class="toc-number">1.4.</span> <span class="toc-text">降低log级别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#禁止cookies"><span class="toc-number">1.5.</span> <span class="toc-text">禁止cookies</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#禁止重试"><span class="toc-number">1.6.</span> <span class="toc-text">禁止重试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#减小下载超时"><span class="toc-number">1.7.</span> <span class="toc-text">减小下载超时</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#禁止重定向"><span class="toc-number">1.8.</span> <span class="toc-text">禁止重定向</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#启用-“Ajax-Crawlable-Pages”-爬取"><span class="toc-number">1.9.</span> <span class="toc-text">启用 “Ajax Crawlable Pages” 爬取</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#借助Firefox来爬取"><span class="toc-number">2.</span> <span class="toc-text">借助Firefox来爬取</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#在浏览器中检查DOM的注意事项"><span class="toc-number">2.1.</span> <span class="toc-text">在浏览器中检查DOM的注意事项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#对爬取有帮助的实用Firefox插件"><span class="toc-number">2.2.</span> <span class="toc-text">对爬取有帮助的实用Firefox插件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Firebug"><span class="toc-number">2.2.1.</span> <span class="toc-text">Firebug</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XPather"><span class="toc-number">2.2.2.</span> <span class="toc-text">XPather</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XPath-Checker"><span class="toc-number">2.2.3.</span> <span class="toc-text">XPath Checker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tamper-Data"><span class="toc-number">2.2.4.</span> <span class="toc-text">Tamper Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Firecookie"><span class="toc-number">2.2.5.</span> <span class="toc-text">Firecookie</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#使用Firebug进行爬取"><span class="toc-number">3.</span> <span class="toc-text">使用Firebug进行爬取</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#介绍"><span class="toc-number">3.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#获取到跟进-follow-的链接"><span class="toc-number">3.2.</span> <span class="toc-text">获取到跟进(follow)的链接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#提取数据"><span class="toc-number">3.3.</span> <span class="toc-text">提取数据</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#调试内存溢出"><span class="toc-number">4.</span> <span class="toc-text">调试内存溢出</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#内存泄露的常见原因"><span class="toc-number">4.1.</span> <span class="toc-text">内存泄露的常见原因</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#请求过多？"><span class="toc-number">4.1.1.</span> <span class="toc-text">请求过多？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用-trackref-调试内存泄露"><span class="toc-number">4.2.</span> <span class="toc-text">使用 trackref 调试内存泄露</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#哪些对象被追踪了"><span class="toc-number">4.2.1.</span> <span class="toc-text">哪些对象被追踪了?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#真实例子"><span class="toc-number">4.2.2.</span> <span class="toc-text">真实例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#很多spider"><span class="toc-number">4.2.3.</span> <span class="toc-text">很多spider?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy-utils-trackref模块"><span class="toc-number">4.2.4.</span> <span class="toc-text">scrapy.utils.trackref模块</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用Guppy调试内存泄露"><span class="toc-number">4.3.</span> <span class="toc-text">使用Guppy调试内存泄露</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Leaks-without-leaks"><span class="toc-number">4.4.</span> <span class="toc-text">Leaks without leaks</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#下载和处理文件和图像"><span class="toc-number">5.</span> <span class="toc-text">下载和处理文件和图像</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#使用文件管道"><span class="toc-number">5.1.</span> <span class="toc-text">使用文件管道</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用图像管道"><span class="toc-number">5.2.</span> <span class="toc-text">使用图像管道</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#启用媒体管道"><span class="toc-number">5.3.</span> <span class="toc-text">启用媒体管道</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#支持的存储"><span class="toc-number">5.4.</span> <span class="toc-text">支持的存储</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#文件系统存储"><span class="toc-number">5.4.1.</span> <span class="toc-text">文件系统存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Amazon-S3存储"><span class="toc-number">5.4.2.</span> <span class="toc-text">Amazon S3存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Google云端存储"><span class="toc-number">5.4.3.</span> <span class="toc-text">Google云端存储</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#附加功能"><span class="toc-number">5.5.</span> <span class="toc-text">附加功能</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#文件到期"><span class="toc-number">5.5.1.</span> <span class="toc-text">文件到期</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为图像生成缩略图"><span class="toc-number">5.5.2.</span> <span class="toc-text">为图像生成缩略图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#过滤掉小图片"><span class="toc-number">5.5.3.</span> <span class="toc-text">过滤掉小图片</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#允许重定向"><span class="toc-number">5.5.4.</span> <span class="toc-text">允许重定向</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#扩展媒体管道"><span class="toc-number">5.6.</span> <span class="toc-text">扩展媒体管道</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自定义图像管道示例"><span class="toc-number">5.7.</span> <span class="toc-text">自定义图像管道示例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#部署Spider"><span class="toc-number">6.</span> <span class="toc-text">部署Spider</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#部署到Scrapyd服务器"><span class="toc-number">6.1.</span> <span class="toc-text">部署到Scrapyd服务器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#部署到Scrapy-Cloud"><span class="toc-number">6.2.</span> <span class="toc-text">部署到Scrapy Cloud</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#自动限速-AutoThrottle-扩展"><span class="toc-number">7.</span> <span class="toc-text">自动限速(AutoThrottle)扩展</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#设计目标"><span class="toc-number">7.1.</span> <span class="toc-text">设计目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#怎么运行的"><span class="toc-number">7.2.</span> <span class="toc-text">怎么运行的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#节流算法"><span class="toc-number">7.3.</span> <span class="toc-text">节流算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#设置"><span class="toc-number">7.4.</span> <span class="toc-text">设置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AUTOTHROTTLE-ENABLED"><span class="toc-number">7.4.1.</span> <span class="toc-text">AUTOTHROTTLE_ENABLED</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AUTOTHROTTLE-START-DELAY"><span class="toc-number">7.4.2.</span> <span class="toc-text">AUTOTHROTTLE_START_DELAY</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AUTOTHROTTLE-MAX-DELAY"><span class="toc-number">7.4.3.</span> <span class="toc-text">AUTOTHROTTLE_MAX_DELAY</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AUTOTHROTTLE-TARGET-CONCURRENCY"><span class="toc-number">7.4.4.</span> <span class="toc-text">AUTOTHROTTLE_TARGET_CONCURRENCY</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AUTOTHROTTLE-DEBUG"><span class="toc-number">7.4.5.</span> <span class="toc-text">AUTOTHROTTLE_DEBUG</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Benchmarking"><span class="toc-number">8.</span> <span class="toc-text">Benchmarking</span></a></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 42
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/07</span><a class="archive-post-title" href= "/2019/01/07/35c3部分做题记录/" >35C3 CTF 部分解题记录（junior）</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2018/12/23/寒假学习计划/" >寒假学习计划</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/12</span><a class="archive-post-title" href= "/2018/09/12/YXcms分析/" >YXCMS 1.4.7</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/12</span><a class="archive-post-title" href= "/2018/09/12/YXcms/" >YXCMS 1.4.7 - Arbitrary file deletion</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span><a class="archive-post-title" href= "/2018/09/07/反序列化之phar学习/" >反序列化之phar://学习</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/06</span><a class="archive-post-title" href= "/2018/09/06/后门/" >ssh后门</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/03</span><a class="archive-post-title" href= "/2018/09/03/git/" >Git</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/01</span><a class="archive-post-title" href= "/2018/09/01/python装饰器/" >python装饰器</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/30</span><a class="archive-post-title" href= "/2018/08/30/hackme/" >hackme CTF 记录</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/18</span><a class="archive-post-title" href= "/2018/08/18/bugku sql2/" >Bugku sql注入2</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/17</span><a class="archive-post-title" href= "/2018/08/17/Hack the ch4inrulz of Vulnhub/" >[Untitled Post]</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/07</span><a class="archive-post-title" href= "/2018/08/07/Cyberry/" >Vulnhub\|Cyberry靶机</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/05</span><a class="archive-post-title" href= "/2018/07/05/暑假学习计划/" >暑假学习计划</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/29</span><a class="archive-post-title" href= "/2018/06/29/xxe学习/" >浅谈XML实体注入漏洞</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/23</span><a class="archive-post-title" href= "/2018/06/23/Bsides靶机/" >Vulnhub Bsides靶机</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/11</span><a class="archive-post-title" href= "/2018/06/11/Linux/" >Linux</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/06</span><a class="archive-post-title" href= "/2018/06/06/超级玛丽/" >超级玛丽靶机渗透测试笔记</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/17</span><a class="archive-post-title" href= "/2018/04/17/webug wp/" >webug wp</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2018/04/18/Broad Crawls~Benchmarking/" >[Untitled Post]</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/25</span><a class="archive-post-title" href= "/2018/05/25/内网/" >域渗透练习</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/23</span><a class="archive-post-title" href= "/2018/05/23/empire/" >Empire使用总结</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/10</span><a class="archive-post-title" href= "/2018/05/10/jarvisoj wp/" >jarvisoj WP</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/09</span><a class="archive-post-title" href= "/2018/05/09/php反序列化学习/" >ctf一道题目。php反序列化学习</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/03</span><a class="archive-post-title" href= "/2018/05/03/渗透测试/" >记一次kali渗透测试靶机</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/23</span><a class="archive-post-title" href= "/2018/04/23/实验吧/" >实验吧WP</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/23</span><a class="archive-post-title" href= "/2018/04/23/i春秋/" >i春秋WP</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/20</span><a class="archive-post-title" href= "/2018/04/20/scrapy爬虫/" >scrapy爬虫实例</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/15</span><a class="archive-post-title" href= "/2018/04/15/线下赛总结/" >线下赛总结</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/10</span><a class="archive-post-title" href= "/2018/04/10/scrapy笔记/" >Scrapy笔记</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/10</span><a class="archive-post-title" href= "/2018/04/10/1个月学习计划/" >1个月学习计划</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/01</span><a class="archive-post-title" href= "/2018/04/01/代码执行/" >rnpnet tiki-calendar.php页面存在代码执行漏洞</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/23</span><a class="archive-post-title" href= "/2018/03/23/VehicleWorkshop/" >VehicleWorkshop上传漏洞本地复现</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/16</span><a class="archive-post-title" href= "/2018/03/16/redtiger level4/" >Redtiger题目</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/15</span><a class="archive-post-title" href= "/2018/03/15/会员日Pwnhub复现/" >会员日Pwnhub题目的复现</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2018/10/06/destoon/" >destoon前台getshell复现</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2018/03/28/cookie与session/" >cookie与session的笔记</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2018/08/25/php-security-calendar-2017/" >php-security-calendar-2017</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2018/06/29/The game of bob/" >Bob 1.0.1</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2018/08/25/eyou前台getshell/" >eyoucms前台getshell复现</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2019/01/09/Code_breaking/" >Code-Breaking Puzzles</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2018/08/23/Hooskcms/" >Hooskcms 几个小洞</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> Invalid date </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">Invalid date</span><a class="archive-post-title" href= "/2018/07/17/JAVA学习情况/" >java学习笔记</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="write-up"><span class="iconfont-archer">&#xe60a;</span>write-up</span>
    
        <span class="sidebar-category-name" data-categories="学习计划"><span class="iconfont-archer">&#xe60a;</span>学习计划</span>
    
        <span class="sidebar-category-name" data-categories="渗透测试"><span class="iconfont-archer">&#xe60a;</span>渗透测试</span>
    
        <span class="sidebar-category-name" data-categories="代码审计"><span class="iconfont-archer">&#xe60a;</span>代码审计</span>
    
        <span class="sidebar-category-name" data-categories="Arbitrary-File-Upload"><span class="iconfont-archer">&#xe60a;</span>Arbitrary-File-Upload</span>
    
        <span class="sidebar-category-name" data-categories="学习笔记"><span class="iconfont-archer">&#xe60a;</span>学习笔记</span>
    
        <span class="sidebar-category-name" data-categories="CVE-Request"><span class="iconfont-archer">&#xe60a;</span>CVE-Request</span>
    
        <span class="sidebar-category-name" data-categories="漏洞分析"><span class="iconfont-archer">&#xe60a;</span>漏洞分析</span>
    
        <span class="sidebar-category-name" data-categories="Linux"><span class="iconfont-archer">&#xe60a;</span>Linux</span>
    
        <span class="sidebar-category-name" data-categories="http协议"><span class="iconfont-archer">&#xe60a;</span>http协议</span>
    
        <span class="sidebar-category-name" data-categories="Empire"><span class="iconfont-archer">&#xe60a;</span>Empire</span>
    
        <span class="sidebar-category-name" data-categories="sql注入"><span class="iconfont-archer">&#xe60a;</span>sql注入</span>
    
        <span class="sidebar-category-name" data-categories="git"><span class="iconfont-archer">&#xe60a;</span>git</span>
    
        <span class="sidebar-category-name" data-categories="php反序列化"><span class="iconfont-archer">&#xe60a;</span>php反序列化</span>
    
        <span class="sidebar-category-name" data-categories="python"><span class="iconfont-archer">&#xe60a;</span>python</span>
    
        <span class="sidebar-category-name" data-categories="scrapy"><span class="iconfont-archer">&#xe60a;</span>scrapy</span>
    
        <span class="sidebar-category-name" data-categories="xxe"><span class="iconfont-archer">&#xe60a;</span>xxe</span>
    
        <span class="sidebar-category-name" data-categories="内网"><span class="iconfont-archer">&#xe60a;</span>内网</span>
    
        <span class="sidebar-category-name" data-categories="代码执行"><span class="iconfont-archer">&#xe60a;</span>代码执行</span>
    
        <span class="sidebar-category-name" data-categories="后门"><span class="iconfont-archer">&#xe60a;</span>后门</span>
    
        <span class="sidebar-category-name" data-categories="线下赛"><span class="iconfont-archer">&#xe60a;</span>线下赛</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "Hu3sky"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    </body>
</html>


